"""
Celery ä»»åŠ¡å®šä¹‰
åŒ…å«å›æµ‹ã€å› å­è¯„ä¼°ã€å› å­ç”Ÿæˆç­‰å¼‚æ­¥ä»»åŠ¡
æ”¯æŒé‡è¯•æœºåˆ¶å’ŒçŠ¶æ€è¿½è¸ª
"""

import logging
from datetime import datetime
from typing import Any
from celery import shared_task
from celery.exceptions import MaxRetriesExceededError

from .app import celery_app

logger = logging.getLogger(__name__)


class TaskError(Exception):
    """ä»»åŠ¡æ‰§è¡Œé”™è¯¯"""
    pass


class RetryableError(Exception):
    """å¯é‡è¯•çš„é”™è¯¯"""
    pass


@celery_app.task(
    bind=True,
    name="iqfmp.celery_app.tasks.backtest_task",
    max_retries=3,
    default_retry_delay=60,
    autoretry_for=(RetryableError,),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
    acks_late=True,
    track_started=True,
    priority=3,
)
def backtest_task(
    self,
    strategy_id: str,
    config: dict[str, Any],
) -> dict[str, Any]:
    """
    ç­–ç•¥å›æµ‹ä»»åŠ¡

    Args:
        strategy_id: ç­–ç•¥ ID
        config: å›æµ‹é…ç½®
            - start_date: å¼€å§‹æ—¥æœŸ
            - end_date: ç»“æŸæ—¥æœŸ
            - initial_capital: åˆå§‹èµ„é‡‘
            - symbols: äº¤æ˜“æ ‡çš„åˆ—è¡¨
            - frequency: æ•°æ®é¢‘ç‡

    Returns:
        å›æµ‹ç»“æœ
            - task_id: ä»»åŠ¡ ID
            - strategy_id: ç­–ç•¥ ID
            - metrics: ç»©æ•ˆæŒ‡æ ‡
            - trades: äº¤æ˜“è®°å½•
    """
    task_id = self.request.id
    logger.info(f"Starting backtest task {task_id} for strategy {strategy_id}")

    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        self.update_state(
            state="PROGRESS",
            meta={
                "current": 0,
                "total": 100,
                "status": "Initializing backtest...",
                "started_at": datetime.utcnow().isoformat(),
            },
        )

        # æ¨¡æ‹Ÿå›æµ‹æ‰§è¡Œè¿‡ç¨‹
        # å®é™…å®ç°ä¸­ä¼šè°ƒç”¨ strategy.backtest æ¨¡å—
        result = _execute_backtest(self, strategy_id, config)

        logger.info(f"Backtest task {task_id} completed successfully")
        return {
            "task_id": task_id,
            "strategy_id": strategy_id,
            "status": "completed",
            "completed_at": datetime.utcnow().isoformat(),
            "result": result,
        }

    except RetryableError as e:
        logger.warning(f"Backtest task {task_id} failed with retryable error: {e}")
        raise self.retry(exc=e)

    except MaxRetriesExceededError:
        logger.error(f"Backtest task {task_id} exceeded max retries")
        return {
            "task_id": task_id,
            "strategy_id": strategy_id,
            "status": "failed",
            "error": "Max retries exceeded",
        }

    except Exception as e:
        logger.error(f"Backtest task {task_id} failed: {e}")
        return {
            "task_id": task_id,
            "strategy_id": strategy_id,
            "status": "failed",
            "error": str(e),
        }


@celery_app.task(
    bind=True,
    name="iqfmp.celery_app.tasks.evaluate_factor_task",
    max_retries=3,
    default_retry_delay=30,
    autoretry_for=(RetryableError,),
    retry_backoff=True,
    acks_late=True,
    track_started=True,
    priority=5,
)
def evaluate_factor_task(
    self,
    factor_id: str,
    factor_code: str,
    evaluation_config: dict[str, Any],
) -> dict[str, Any]:
    """
    å› å­è¯„ä¼°ä»»åŠ¡

    Args:
        factor_id: å› å­ ID
        factor_code: å› å­ä»£ç 
        evaluation_config: è¯„ä¼°é…ç½®
            - cv_splits: äº¤å‰éªŒè¯åˆ‡åˆ†æ•°
            - metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨

    Returns:
        è¯„ä¼°ç»“æœ
            - factor_id: å› å­ ID
            - metrics: IC/IR/Sharpe ç­‰æŒ‡æ ‡
            - stability: ç¨³å®šæ€§åˆ†æ
    """
    task_id = self.request.id
    logger.info(f"Starting factor evaluation task {task_id} for factor {factor_id}")

    try:
        self.update_state(
            state="PROGRESS",
            meta={
                "current": 0,
                "total": 100,
                "status": "Loading factor data...",
                "started_at": datetime.utcnow().isoformat(),
            },
        )

        # æ¨¡æ‹Ÿå› å­è¯„ä¼°è¿‡ç¨‹
        result = _execute_factor_evaluation(self, factor_id, factor_code, evaluation_config)

        logger.info(f"Factor evaluation task {task_id} completed")
        return {
            "task_id": task_id,
            "factor_id": factor_id,
            "status": "completed",
            "completed_at": datetime.utcnow().isoformat(),
            "result": result,
        }

    except RetryableError as e:
        logger.warning(f"Factor evaluation task {task_id} failed with retryable error: {e}")
        raise self.retry(exc=e)

    except Exception as e:
        logger.error(f"Factor evaluation task {task_id} failed: {e}")
        return {
            "task_id": task_id,
            "factor_id": factor_id,
            "status": "failed",
            "error": str(e),
        }


@celery_app.task(
    bind=True,
    name="iqfmp.celery_app.tasks.generate_factor_task",
    max_retries=2,
    default_retry_delay=30,
    acks_late=True,
    track_started=True,
    priority=5,
)
def generate_factor_task(
    self,
    hypothesis: str,
    factor_family: str,
    generation_config: dict[str, Any],
) -> dict[str, Any]:
    """
    å› å­ç”Ÿæˆä»»åŠ¡

    Args:
        hypothesis: å› å­å‡è®¾æè¿°
        factor_family: å› å­å®¶æ—
        generation_config: ç”Ÿæˆé…ç½®

    Returns:
        ç”Ÿæˆç»“æœ
            - factor_id: æ–°ç”Ÿæˆçš„å› å­ ID
            - code: å› å­ä»£ç 
            - metadata: å› å­å…ƒæ•°æ®
    """
    task_id = self.request.id
    logger.info(f"Starting factor generation task {task_id}")

    try:
        self.update_state(
            state="PROGRESS",
            meta={
                "current": 0,
                "total": 100,
                "status": "Calling LLM to generate factor...",
                "started_at": datetime.utcnow().isoformat(),
            },
        )

        # æ¨¡æ‹Ÿå› å­ç”Ÿæˆè¿‡ç¨‹
        result = _execute_factor_generation(self, hypothesis, factor_family, generation_config)

        logger.info(f"Factor generation task {task_id} completed")
        return {
            "task_id": task_id,
            "status": "completed",
            "completed_at": datetime.utcnow().isoformat(),
            "result": result,
        }

    except Exception as e:
        logger.error(f"Factor generation task {task_id} failed: {e}")
        return {
            "task_id": task_id,
            "status": "failed",
            "error": str(e),
        }


@celery_app.task(
    bind=True,
    name="iqfmp.celery_app.tasks.batch_backtest_task",
    max_retries=1,
    acks_late=True,
    track_started=True,
    priority=2,
)
def batch_backtest_task(
    self,
    strategies: list[dict[str, Any]],
    common_config: dict[str, Any],
) -> dict[str, Any]:
    """
    æ‰¹é‡å›æµ‹ä»»åŠ¡

    Args:
        strategies: ç­–ç•¥åˆ—è¡¨
        common_config: é€šç”¨é…ç½®

    Returns:
        æ‰¹é‡å›æµ‹ç»“æœ
    """
    task_id = self.request.id
    logger.info(f"Starting batch backtest task {task_id} for {len(strategies)} strategies")

    results = []
    total = len(strategies)

    for i, strategy in enumerate(strategies):
        self.update_state(
            state="PROGRESS",
            meta={
                "current": i,
                "total": total,
                "status": f"Running backtest {i + 1}/{total}...",
            },
        )

        try:
            result = _execute_backtest(self, strategy["id"], {**common_config, **strategy.get("config", {})})
            results.append({
                "strategy_id": strategy["id"],
                "status": "completed",
                "result": result,
            })
        except Exception as e:
            results.append({
                "strategy_id": strategy["id"],
                "status": "failed",
                "error": str(e),
            })

    return {
        "task_id": task_id,
        "status": "completed",
        "total": total,
        "successful": sum(1 for r in results if r["status"] == "completed"),
        "failed": sum(1 for r in results if r["status"] == "failed"),
        "results": results,
    }


@celery_app.task(
    bind=True,
    name="iqfmp.celery_app.tasks.risk_check_task",
    max_retries=0,
    acks_late=False,
    track_started=True,
    priority=9,
)
def risk_check_task(
    self,
    account_id: str,
    check_config: dict[str, Any],
) -> dict[str, Any]:
    """
    é£æ§æ£€æŸ¥ä»»åŠ¡ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰- ä½¿ç”¨çœŸå®é£æ§å¼•æ“

    ä½¿ç”¨ RiskController æ‰§è¡Œå››å±‚é£é™©æ£€æŸ¥ï¼š
    1. å›æ’¤æ£€æŸ¥ - MAX_DRAWDOWN_THRESHOLD (15%)
    2. æŒä»“é›†ä¸­åº¦æ£€æŸ¥ - MAX_POSITION_RATIO (30%)
    3. æ æ†æ£€æŸ¥ - MAX_LEVERAGE (3x)
    4. å•æ—¥äºæŸæ£€æŸ¥ - EMERGENCY_LOSS_THRESHOLD (5%)

    Args:
        account_id: è´¦æˆ· ID
        check_config: æ£€æŸ¥é…ç½®
            - equity: å½“å‰æƒç›Š
            - peak_equity: å³°å€¼æƒç›Š
            - total_position_value: æ€»ä»“ä½ä»·å€¼
            - daily_pnl: å½“æ—¥ç›ˆäº
            - positions: æŒä»“åˆ—è¡¨ [{"symbol": str, "value": float}]

    Returns:
        é£æ§æ£€æŸ¥ç»“æœ
    """
    from decimal import Decimal
    from iqfmp.exchange.risk import (
        Account,
        Position,
        RiskConfig,
        RiskController,
        RiskLevel,
    )

    task_id = self.request.id
    logger.info(f"Running risk check task {task_id} for account {account_id}")

    # ä» check_config è§£æè´¦æˆ·æ•°æ®
    equity = Decimal(str(check_config.get("equity", 100000)))
    peak_equity = Decimal(str(check_config.get("peak_equity", equity)))
    total_position_value = Decimal(str(check_config.get("total_position_value", 0)))
    daily_pnl = Decimal(str(check_config.get("daily_pnl", 0)))
    positions_data = check_config.get("positions", [])

    # æ„å»º Account å¯¹è±¡
    account = Account(
        equity=equity,
        total_position_value=total_position_value,
        daily_pnl=daily_pnl,
        peak_equity=peak_equity,
    )

    # åˆå§‹åŒ–é£æ§æ§åˆ¶å™¨
    risk_config = RiskConfig()
    controller = RiskController(config=risk_config, initial_equity=peak_equity)
    controller.update_equity(equity)

    # æ·»åŠ æŒä»“åˆ°æ§åˆ¶å™¨
    for pos in positions_data:
        symbol = pos.get("symbol", "UNKNOWN")
        value = Decimal(str(pos.get("value", 0)))
        controller.add_position(symbol, value)

    # æ‰§è¡Œé£æ§æ£€æŸ¥ - é€ä¸ªæŒä»“æ£€æŸ¥
    all_violations = []
    checks_result = {}

    # æ£€æŸ¥æ¯ä¸ªæŒä»“
    for pos in positions_data:
        symbol = pos.get("symbol", "UNKNOWN")
        value = Decimal(str(pos.get("value", 0)))
        position = Position(symbol=symbol, value=value)
        result = controller.check_risk_sync(position, account)
        all_violations.extend(result.violations)

    # æ±‡æ€»æ£€æŸ¥ç»“æœ
    # 1. å›æ’¤æ£€æŸ¥
    drawdown = (peak_equity - equity) / peak_equity if peak_equity > 0 else Decimal("0")
    drawdown_threshold = RiskController.MAX_DRAWDOWN_THRESHOLD
    drawdown_ok = drawdown <= drawdown_threshold
    checks_result["max_drawdown"] = {
        "status": "ok" if drawdown_ok else "breach",
        "value": float(drawdown * 100),
        "threshold": float(drawdown_threshold * 100),
        "message": f"å›æ’¤ {drawdown * 100:.2f}% {'æ­£å¸¸' if drawdown_ok else 'è¶…é™'}",
    }

    # 2. æ æ†æ£€æŸ¥
    leverage = total_position_value / equity if equity > 0 else Decimal("0")
    leverage_threshold = RiskController.MAX_LEVERAGE
    leverage_ok = leverage <= leverage_threshold
    checks_result["leverage"] = {
        "status": "ok" if leverage_ok else "breach",
        "value": float(leverage),
        "threshold": float(leverage_threshold),
        "message": f"æ æ† {leverage:.2f}x {'æ­£å¸¸' if leverage_ok else 'è¶…é™'}",
    }

    # 3. å•æ—¥äºæŸæ£€æŸ¥
    if daily_pnl < 0 and equity > 0:
        daily_loss_ratio = -daily_pnl / equity
    else:
        daily_loss_ratio = Decimal("0")
    daily_loss_threshold = RiskController.EMERGENCY_LOSS_THRESHOLD
    daily_loss_ok = daily_loss_ratio <= daily_loss_threshold
    checks_result["daily_loss"] = {
        "status": "ok" if daily_loss_ok else "breach",
        "value": float(daily_loss_ratio * 100),
        "threshold": float(daily_loss_threshold * 100),
        "message": f"å•æ—¥äºæŸ {daily_loss_ratio * 100:.2f}% {'æ­£å¸¸' if daily_loss_ok else 'è¶…é™'}",
    }

    # 4. æŒä»“é›†ä¸­åº¦æ£€æŸ¥
    max_concentration = Decimal("0")
    concentration_threshold = RiskController.MAX_POSITION_RATIO
    for pos in positions_data:
        value = Decimal(str(pos.get("value", 0)))
        if equity > 0:
            conc = value / equity
            if conc > max_concentration:
                max_concentration = conc
    concentration_ok = max_concentration <= concentration_threshold
    checks_result["position_concentration"] = {
        "status": "ok" if concentration_ok else "breach",
        "value": float(max_concentration * 100),
        "threshold": float(concentration_threshold * 100),
        "message": f"æœ€å¤§æŒä»“é›†ä¸­åº¦ {max_concentration * 100:.2f}% {'æ­£å¸¸' if concentration_ok else 'è¶…é™'}",
    }

    # ç¡®å®šæ•´ä½“é£é™©ç­‰çº§
    is_safe = all(c["status"] == "ok" for c in checks_result.values())
    has_critical = any(v.severity == "critical" for v in all_violations)
    has_high = any(v.severity == "high" for v in all_violations)

    if has_critical:
        risk_level = "critical"
        recommended_action = "emergency_close_all"
    elif has_high:
        risk_level = "danger"
        recommended_action = "reduce_position"
    elif not is_safe:
        risk_level = "warning"
        recommended_action = "monitor"
    else:
        risk_level = "normal"
        recommended_action = "none"

    logger.info(f"Risk check completed: level={risk_level}, violations={len(all_violations)}")

    return {
        "task_id": task_id,
        "account_id": account_id,
        "status": "completed",
        "risk_level": risk_level,
        "is_safe": is_safe,
        "recommended_action": recommended_action,
        "checks": checks_result,
        "violations": [
            {
                "type": v.type,
                "severity": v.severity,
                "action": v.action,
                "message": v.message,
                "current_value": float(v.current_value),
                "threshold": float(v.threshold),
            }
            for v in all_violations
        ],
        "thresholds": {
            k: float(v) for k, v in RiskController.get_hard_thresholds().items()
        },
    }


@celery_app.task(
    bind=True,
    name="iqfmp.celery_app.tasks.emergency_close_task",
    max_retries=0,
    acks_late=False,
    track_started=True,
    priority=10,
)
def emergency_close_task(
    self,
    account_id: str,
    positions: list[str],
    reason: str,
) -> dict[str, Any]:
    """
    ç´§æ€¥å¹³ä»“ä»»åŠ¡ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰- è°ƒç”¨çœŸå®äº¤æ˜“æ‰€ API

    é€šè¿‡ ccxt è¿æ¥äº¤æ˜“æ‰€æ‰§è¡Œç´§æ€¥å¹³ä»“æ“ä½œã€‚
    å¦‚æœäº¤æ˜“æ‰€å‡­è¯æœªé…ç½®ï¼Œå°†è¿”å›é”™è¯¯å¹¶å‘é€å‘Šè­¦é€šçŸ¥ã€‚

    Args:
        account_id: è´¦æˆ· ID
        positions: æŒä»“æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯:
            - å­—ç¬¦ä¸²: "BTCUSDT" (ä»…ç¬¦å·)
            - å­—å…¸: {"symbol": "BTCUSDT", "side": "long", "quantity": 0.1}
        reason: å¹³ä»“åŸå› 

    Returns:
        å¹³ä»“ç»“æœ
    """
    import asyncio
    import os
    from decimal import Decimal

    task_id = self.request.id
    logger.warning(f"EMERGENCY: Running emergency close task {task_id} for account {account_id}")
    logger.warning(f"Reason: {reason}")
    logger.warning(f"Positions to close: {positions}")

    close_results = []
    telegram_sent = False

    # å°è¯•å‘é€ Telegram å‘Šè­¦
    async def _send_telegram_alert() -> bool:
        """å‘é€ Telegram ç´§æ€¥å‘Šè­¦"""
        try:
            from iqfmp.exchange.emergency import TelegramNotifier

            bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
            chat_id = os.getenv("TELEGRAM_CHAT_ID")

            if not bot_token or not chat_id:
                logger.warning("Telegram credentials not configured")
                return False

            notifier = TelegramNotifier(
                bot_token=bot_token,
                chat_id=chat_id,
                enabled=True,
            )

            # å‘é€ç´§æ€¥å‘Šè­¦
            message = (
                f"ğŸš¨ <b>ç´§æ€¥å¹³ä»“æ‰§è¡Œä¸­</b>\n\n"
                f"<b>è´¦æˆ·</b>: {account_id}\n"
                f"<b>åŸå› </b>: {reason}\n"
                f"<b>æŒä»“æ•°é‡</b>: {len(positions)}\n"
                f"<b>ä»»åŠ¡ID</b>: {task_id}"
            )
            return await notifier._send_message(message)
        except Exception as e:
            logger.error(f"Failed to send Telegram alert: {e}")
            return False

    try:
        telegram_sent = asyncio.run(_send_telegram_alert())
    except Exception as e:
        logger.error(f"Telegram alert error: {e}")

    # æ£€æŸ¥äº¤æ˜“æ‰€å‡­è¯
    exchange_type = os.getenv("EXCHANGE_TYPE", "binance")
    api_key = os.getenv("EXCHANGE_API_KEY", os.getenv("BINANCE_API_KEY"))
    api_secret = os.getenv("EXCHANGE_API_SECRET", os.getenv("BINANCE_API_SECRET"))

    if not api_key or not api_secret:
        logger.error("Exchange credentials not configured - cannot execute real close orders")
        return {
            "task_id": task_id,
            "account_id": account_id,
            "status": "failed",
            "error": "Exchange credentials not configured. Set EXCHANGE_API_KEY and EXCHANGE_API_SECRET.",
            "positions_to_close": positions,
            "reason": reason,
            "telegram_sent": telegram_sent,
            "completed_at": datetime.utcnow().isoformat(),
        }

    # æ‰§è¡ŒçœŸå®å¹³ä»“
    async def _execute_emergency_close() -> list[dict]:
        """å¼‚æ­¥æ‰§è¡Œç´§æ€¥å¹³ä»“"""
        results = []

        try:
            # åŠ¨æ€å¯¼å…¥ ccxt
            import ccxt.async_support as ccxt_async

            # åˆ›å»ºäº¤æ˜“æ‰€è¿æ¥
            exchange_class = getattr(ccxt_async, exchange_type, ccxt_async.binance)
            exchange = exchange_class({
                "apiKey": api_key,
                "secret": api_secret,
                "sandbox": os.getenv("EXCHANGE_SANDBOX", "false").lower() == "true",
                "options": {"defaultType": "future"},
            })

            try:
                # åŠ è½½å¸‚åœºä¿¡æ¯
                await exchange.load_markets()

                for pos_data in positions:
                    # è§£ææŒä»“æ•°æ®
                    if isinstance(pos_data, str):
                        symbol = pos_data
                        quantity = None
                        side = None
                    else:
                        symbol = pos_data.get("symbol", "")
                        quantity = pos_data.get("quantity")
                        side = pos_data.get("side")

                    if not symbol:
                        results.append({
                            "symbol": "UNKNOWN",
                            "status": "failed",
                            "error": "Invalid position data",
                        })
                        continue

                    try:
                        # è·å–å½“å‰æŒä»“
                        positions_info = await exchange.fetch_positions([symbol])
                        position = next((p for p in positions_info if p["symbol"] == symbol and float(p.get("contracts", 0)) != 0), None)

                        if not position:
                            results.append({
                                "symbol": symbol,
                                "status": "skipped",
                                "message": "No open position found",
                            })
                            continue

                        # ç¡®å®šå¹³ä»“æ–¹å‘å’Œæ•°é‡
                        pos_side = position.get("side", "long")
                        pos_contracts = abs(float(position.get("contracts", 0)))

                        if quantity:
                            close_quantity = min(float(quantity), pos_contracts)
                        else:
                            close_quantity = pos_contracts

                        # æ‰§è¡Œå¸‚ä»·å¹³ä»“
                        order_side = "sell" if pos_side == "long" else "buy"
                        order = await exchange.create_market_order(
                            symbol=symbol,
                            side=order_side,
                            amount=close_quantity,
                            params={"reduceOnly": True},
                        )

                        results.append({
                            "symbol": symbol,
                            "status": "success",
                            "order_id": order.get("id"),
                            "side": order_side,
                            "quantity": close_quantity,
                            "average_price": order.get("average"),
                            "filled": order.get("filled"),
                        })
                        logger.info(f"Successfully closed {symbol}: {order}")

                    except Exception as e:
                        logger.error(f"Failed to close {symbol}: {e}")
                        results.append({
                            "symbol": symbol,
                            "status": "failed",
                            "error": str(e),
                        })

            finally:
                await exchange.close()

        except ImportError:
            logger.error("ccxt not installed - cannot execute exchange orders")
            return [{"status": "failed", "error": "ccxt not installed"}]
        except Exception as e:
            logger.error(f"Exchange connection failed: {e}")
            return [{"status": "failed", "error": str(e)}]

        return results

    try:
        close_results = asyncio.run(_execute_emergency_close())
    except Exception as e:
        logger.error(f"Emergency close execution failed: {e}")
        close_results = [{"status": "failed", "error": str(e)}]

    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in close_results if r.get("status") == "success")
    failed_count = sum(1 for r in close_results if r.get("status") == "failed")
    skipped_count = sum(1 for r in close_results if r.get("status") == "skipped")

    # å‘é€ç»“æœé€šçŸ¥
    async def _send_result_notification() -> bool:
        try:
            from iqfmp.exchange.emergency import TelegramNotifier

            bot_token = os.getenv("TELEGRAM_BOT_TOKEN")
            chat_id = os.getenv("TELEGRAM_CHAT_ID")

            if not bot_token or not chat_id:
                return False

            notifier = TelegramNotifier(bot_token=bot_token, chat_id=chat_id, enabled=True)

            status_icon = "âœ…" if failed_count == 0 else "âš ï¸"
            message = (
                f"{status_icon} <b>ç´§æ€¥å¹³ä»“å®Œæˆ</b>\n\n"
                f"<b>æˆåŠŸ</b>: {success_count}\n"
                f"<b>å¤±è´¥</b>: {failed_count}\n"
                f"<b>è·³è¿‡</b>: {skipped_count}\n"
                f"<b>ä»»åŠ¡ID</b>: {task_id}"
            )
            return await notifier._send_message(message)
        except Exception:
            return False

    try:
        asyncio.run(_send_result_notification())
    except Exception:
        pass

    logger.warning(f"Emergency close completed: success={success_count}, failed={failed_count}, skipped={skipped_count}")

    return {
        "task_id": task_id,
        "account_id": account_id,
        "status": "completed" if failed_count == 0 else "partial",
        "reason": reason,
        "results": close_results,
        "summary": {
            "total": len(positions),
            "success": success_count,
            "failed": failed_count,
            "skipped": skipped_count,
        },
        "telegram_sent": telegram_sent,
        "completed_at": datetime.utcnow().isoformat(),
    }


# è¾…åŠ©å‡½æ•°

def _execute_backtest(task, strategy_id: str, config: dict[str, Any]) -> dict[str, Any]:
    """æ‰§è¡Œå›æµ‹çš„å†…éƒ¨å®ç° - ä½¿ç”¨çœŸå®å›æµ‹å¼•æ“"""
    from iqfmp.core.backtest_engine import BacktestEngine
    from iqfmp.core.factor_engine import BUILTIN_FACTORS

    # é˜¶æ®µ1: åˆå§‹åŒ–
    task.update_state(
        state="PROGRESS",
        meta={"current": 10, "total": 100, "status": "Loading market data..."},
    )

    # è·å–å› å­ä»£ç  (ä»é…ç½®æˆ–ä½¿ç”¨é»˜è®¤)
    factor_code = config.get("factor_code")
    if not factor_code:
        # æ ¹æ®ç­–ç•¥ç±»å‹é€‰æ‹©å†…ç½®å› å­
        factor_name = config.get("factor_name", "momentum_20d")
        factor_code = BUILTIN_FACTORS.get(factor_name, BUILTIN_FACTORS["momentum_20d"])

    # é˜¶æ®µ2: åŠ è½½æ•°æ®
    task.update_state(
        state="PROGRESS",
        meta={"current": 30, "total": 100, "status": "Initializing backtest engine..."},
    )

    try:
        engine = BacktestEngine()

        # é˜¶æ®µ3: æ‰§è¡Œå›æµ‹
        task.update_state(
            state="PROGRESS",
            meta={"current": 50, "total": 100, "status": "Running backtest simulation..."},
        )

        result = engine.run_factor_backtest(
            factor_code=factor_code,
            initial_capital=config.get("initial_capital", 100000.0),
            start_date=config.get("start_date"),
            end_date=config.get("end_date"),
            rebalance_frequency=config.get("rebalance_frequency", "daily"),
            position_size=config.get("position_size", 1.0),
            long_only=config.get("long_only", False),
        )

        # é˜¶æ®µ4: å¤„ç†ç»“æœ
        task.update_state(
            state="PROGRESS",
            meta={"current": 90, "total": 100, "status": "Processing results..."},
        )

        return {
            "sharpe_ratio": result.sharpe_ratio,
            "sortino_ratio": result.sortino_ratio,
            "max_drawdown": result.max_drawdown,
            "total_return": result.total_return,
            "annual_return": result.annual_return,
            "win_rate": result.win_rate,
            "total_trades": result.trade_count,
            "profit_factor": result.profit_factor,
            "calmar_ratio": result.calmar_ratio,
            "volatility": result.volatility,
            "avg_trade_return": result.avg_trade_return,
            "avg_holding_period": result.avg_holding_period,
            "monthly_returns": result.monthly_returns,
        }

    except Exception as e:
        logger.error(f"Backtest execution failed: {e}")
        raise TaskError(f"Backtest failed: {e}")


def _execute_factor_evaluation(
    task,
    factor_id: str,
    factor_code: str,
    config: dict[str, Any],
) -> dict[str, Any]:
    """æ‰§è¡Œå› å­è¯„ä¼°çš„å†…éƒ¨å®ç° - ä½¿ç”¨çœŸå®å› å­å¼•æ“"""
    from datetime import datetime as _dt
    from iqfmp.core.factor_engine import FactorEngine, FactorEvaluator, create_engine_with_sample_data
    from iqfmp.core.data_provider import load_ohlcv_sync
    from iqfmp.evaluation.research_ledger import DynamicThreshold, ThresholdConfig

    # é˜¶æ®µ1: åŠ è½½æ•°æ®
    task.update_state(
        state="PROGRESS",
        meta={"current": 20, "total": 100, "status": "Loading market data..."},
    )

    try:
        # åˆ›å»ºå¼•æ“å¹¶åŠ è½½çœŸå®æ•°æ®ï¼ˆä¼˜å…ˆ data_configï¼‰
        data_cfg = config.get("data_config") or {}
        symbol = (data_cfg.get("symbols") or ["ETHUSDT"])[0]
        timeframe = (data_cfg.get("timeframes") or ["1d"])[0]
        start_date = data_cfg.get("start_date")
        end_date = data_cfg.get("end_date")

        df = None
        try:
            df = load_ohlcv_sync(symbol=symbol, timeframe=timeframe, use_db=True)
            if start_date and end_date:
                try:
                    start_dt = _dt.fromisoformat(start_date)
                    end_dt = _dt.fromisoformat(end_date)
                    if start_dt.tzinfo is None:
                        start_dt = start_dt.replace(tzinfo=None)
                    if end_dt.tzinfo is None:
                        end_dt = end_dt.replace(tzinfo=None)
                    df = df[(df["timestamp"] >= start_dt) & (df["timestamp"] <= end_dt)]
                except Exception as sub_e:  # noqa: BLE001
                    logger.warning(f"Date filter failed, using full dataset: {sub_e}")

            # å¦‚æœæä¾›æ—¶é—´åŒºé—´ï¼Œå°è¯•åŠ è½½å«è¡ç”Ÿå“çš„ç»Ÿä¸€æ•°æ®é›†
            if start_date and end_date:
                try:
                    import asyncio
                    from iqfmp.data.provider import UnifiedMarketDataProvider, DataLoadConfig
                    from iqfmp.db.database import get_async_session

                    start_dt = _dt.fromisoformat(start_date)
                    end_dt = _dt.fromisoformat(end_date)

                    async def _load_unified():
                        async with get_async_session() as async_session:
                            provider = UnifiedMarketDataProvider(
                                async_session,
                                exchange=data_cfg.get("exchange", "binance"),
                            )
                            result = await provider.load_market_data(
                                symbol=symbol,
                                start_date=start_dt,
                                end_date=end_dt,
                                timeframe=timeframe,
                                config=DataLoadConfig(
                                    include_derivatives=True,
                                    market_type=data_cfg.get("market_type", "futures"),
                                ),
                            )
                            return result.df

                    unified_df = asyncio.run(_load_unified())
                    if unified_df is not None and len(unified_df) > 0:
                        df = unified_df
                        logger.info("Loaded unified market data with derivatives for evaluation")
                except Exception as sub_e:  # noqa: BLE001
                    logger.warning(f"Unified derivative data load failed, fallback to OHLCV only: {sub_e}")
        except Exception as e:  # noqa: BLE001
            logger.warning(f"Real data load failed, fallback to sample data: {e}")
            df = None

        if df is not None and not df.empty:
            engine = FactorEngine(df=df)
        else:
            engine = create_engine_with_sample_data()

        # é˜¶æ®µ2: è®¡ç®—å› å­å€¼
        task.update_state(
            state="PROGRESS",
            meta={"current": 40, "total": 100, "status": "Computing factor values..."},
        )

        factor_values = engine.compute_factor(factor_code, factor_name=factor_id)

        # é˜¶æ®µ3: è¯„ä¼°å› å­
        task.update_state(
            state="PROGRESS",
            meta={"current": 60, "total": 100, "status": "Evaluating factor metrics..."},
        )

        evaluator = FactorEvaluator(engine)
        eval_result = evaluator.evaluate(
            factor_values=factor_values,
            forward_periods=config.get("forward_periods", [1, 5, 10]),
        )

        # é˜¶æ®µ4: è®¡ç®—åŠ¨æ€é˜ˆå€¼
        task.update_state(
            state="PROGRESS",
            meta={"current": 80, "total": 100, "status": "Checking threshold..."},
        )

        # è·å–å½“å‰è¯•éªŒæ•°è®¡ç®—é˜ˆå€¼
        n_trials = config.get("n_trials", 1)
        threshold_calc = DynamicThreshold(ThresholdConfig())
        current_threshold = threshold_calc.calculate(n_trials)

        metrics = eval_result["metrics"]
        sharpe = metrics.get("sharpe", 0.0)
        passed_threshold = sharpe >= current_threshold

        # é˜¶æ®µ5: æ±‡æ€»ç»“æœ
        task.update_state(
            state="PROGRESS",
            meta={"current": 95, "total": 100, "status": "Finalizing results..."},
        )

        # è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
        stability = eval_result.get("stability", {})
        time_stability = stability.get("time_stability", {})
        market_stability = stability.get("market_stability", {})
        stability_score = (
            abs(time_stability.get("monthly_ic_stability", 0)) +
            abs(market_stability.get("overall", 0))
        ) / 2

        return {
            "ic": metrics.get("ic_mean", 0.0),
            "ic_std": metrics.get("ic_std", 0.0),
            "ic_ir": metrics.get("ir", 0.0),
            "sharpe": sharpe,
            "max_drawdown": metrics.get("max_drawdown", 0.0),
            "total_return": metrics.get("total_return", 0.0),
            "win_rate": metrics.get("win_rate", 0.0),
            "turnover": metrics.get("turnover", 0.0),
            "ic_by_split": metrics.get("ic_by_split", {}),
            "sharpe_by_split": metrics.get("sharpe_by_split", {}),
            "stability_score": round(stability_score, 4),
            "threshold_used": round(current_threshold, 4),
            "passed_threshold": passed_threshold,
        }

    except Exception as e:
        logger.error(f"Factor evaluation failed: {e}")
        raise TaskError(f"Factor evaluation failed: {e}")


def _execute_factor_generation(
    task,
    hypothesis: str,
    factor_family: str,
    config: dict[str, Any],
) -> dict[str, Any]:
    """æ‰§è¡Œå› å­ç”Ÿæˆçš„å†…éƒ¨å®ç° - è°ƒç”¨çœŸå® LLM

    ä½¿ç”¨ FactorGenerationAgent é€šè¿‡ OpenRouter API ç”ŸæˆçœŸå®çš„å› å­ä»£ç ã€‚
    åŒ…å« AST å®‰å…¨æ£€æŸ¥ã€å­—æ®µçº¦æŸéªŒè¯å’Œ**é”™è¯¯åé¦ˆå¾ªç¯**ã€‚

    é”™è¯¯åé¦ˆå¾ªç¯æœºåˆ¶ï¼š
    1. åˆæ¬¡ç”Ÿæˆå› å­
    2. å¦‚æœç”Ÿæˆæˆ–è®¡ç®—å¤±è´¥ï¼Œå°†é”™è¯¯ä¿¡æ¯åé¦ˆç»™ LLM
    3. LLM æ ¹æ®é”™è¯¯ä¿¡æ¯ç”Ÿæˆæ”¹è¿›ç‰ˆæœ¬
    4. æœ€å¤šé‡è¯• max_retries æ¬¡
    """
    import asyncio
    import uuid

    from iqfmp.agents.factor_generation import (
        FactorFamily,
        FactorGenerationAgent,
        FactorGenerationConfig,
        FactorGenerationError,
        InvalidFactorError,
    )
    from iqfmp.llm.provider import LLMConfig, LLMProvider

    task.update_state(
        state="PROGRESS",
        meta={"current": 10, "total": 100, "status": "Initializing LLM provider..."},
    )

    # ä¼˜å…ˆä» ConfigService é…ç½®æ–‡ä»¶è¯»å–é…ç½®ï¼ˆç”¨æˆ·åœ¨å‰ç«¯é…ç½®çš„ï¼‰
    # Celery worker æ˜¯ç‹¬ç«‹è¿›ç¨‹ï¼Œæ— æ³•ç»§æ‰¿ FastAPI è¿›ç¨‹çš„ç¯å¢ƒå˜é‡
    # å› æ­¤éœ€è¦ç›´æ¥ä»é…ç½®æ–‡ä»¶ ~/.iqfmp/config.json è¯»å–
    # æ³¨æ„ï¼šæ¯æ¬¡ä»»åŠ¡æ‰§è¡Œéƒ½é‡æ–°è¯»å–ï¼Œç¡®ä¿å‰ç«¯ä¿®æ”¹èƒ½åŠæ—¶ç”Ÿæ•ˆ
    import json
    from pathlib import Path

    config_file = Path.home() / ".iqfmp" / "config.json"
    api_key_from_config = None
    if config_file.exists():
        try:
            with open(config_file) as f:
                saved_config = json.load(f)
                api_key_from_config = saved_config.get("api_key")
        except Exception as e:
            logger.warning(f"Failed to read config file: {e}")

    # å¦‚æœä»é…ç½®æ–‡ä»¶è·å–åˆ° API keyï¼Œè®¾ç½®åˆ°ç¯å¢ƒå˜é‡ä¾› LLMConfig.from_env() ä½¿ç”¨
    if api_key_from_config:
        import os
        os.environ["OPENROUTER_API_KEY"] = api_key_from_config
        logger.info("Loaded API key from config file ~/.iqfmp/config.json")

    # å¼ºåˆ¶é‡æ–°åŠ è½½ AgentModelRegistryï¼Œç¡®ä¿è·å–å‰ç«¯æœ€æ–°çš„æ¨¡å‹é…ç½®
    # è¿™æ ·ç”¨æˆ·åœ¨å‰ç«¯åˆ‡æ¢æ¨¡å‹åï¼Œä¸‹æ¬¡ä»»åŠ¡ä¼šä½¿ç”¨æ–°æ¨¡å‹
    from iqfmp.agents.model_config import reload_model_registry
    reload_model_registry()
    logger.info("Reloaded agent model registry from config file")

    # å°è¯•ä»ç¯å¢ƒå˜é‡åŠ è½½ LLM é…ç½®
    try:
        llm_config = LLMConfig.from_env()
    except ValueError as e:
        # å¦‚æœæ²¡æœ‰é…ç½® API keyï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›é™çº§ç»“æœ
        logger.warning(f"LLM config error: {e}. Factor generation will use fallback.")
        return {
            "factor_id": str(uuid.uuid4()),
            "hypothesis": hypothesis,
            "family": factor_family,
            "code": f"# LLM not configured - placeholder factor\n# Hypothesis: {hypothesis}\ndef calculate(data):\n    return data['close'].pct_change()",
            "warning": "LLM not configured. Please configure OpenRouter API key in Settings page.",
        }

    task.update_state(
        state="PROGRESS",
        meta={"current": 20, "total": 100, "status": "Connecting to LLM..."},
    )

    # æ˜ å°„ factor_family å­—ç¬¦ä¸²åˆ°æšä¸¾
    family_map = {f.value: f for f in FactorFamily}
    factor_family_enum = family_map.get(factor_family.lower(), FactorFamily.MOMENTUM)

    # åˆ›å»º agent é…ç½®
    max_retries = config.get("max_retries", 5)
    agent_config = FactorGenerationConfig(
        name="celery_factor_generator",
        security_check_enabled=True,
        field_constraint_enabled=True,
        max_retries=max_retries,
        include_examples=True,
    )

    async def _generate_with_feedback_loop() -> dict[str, Any]:
        """å¼‚æ­¥ç”Ÿæˆå› å­ï¼ŒåŒ…å«æ™ºèƒ½æŒ‡æ ‡åé¦ˆå¾ªç¯

        Intelligent Feedback Loop:
        1. Parse user hypothesis to extract requested indicators
        2. Generate factor expression
        3. Check if all indicators are implemented
        4. If missing, provide specific feedback and retry
        5. Continue until complete or max retries reached
        """
        # Import indicator intelligence module
        from iqfmp.agents.indicator_intelligence import (
            check_factor_completeness,
            get_indicator_summary,
            analyze_indicator_coverage,
        )

        async with LLMProvider(llm_config) as llm:
            agent = FactorGenerationAgent(config=agent_config, llm_provider=llm)

            last_code = None
            last_error = None

            # Log detected indicators from hypothesis
            indicator_summary = get_indicator_summary(hypothesis)
            logger.info(f"Factor generation - {indicator_summary}")

            for attempt in range(max_retries):
                try:
                    task.update_state(
                        state="PROGRESS",
                        meta={
                            "current": 30 + attempt * 15,
                            "total": 100,
                            "status": f"Generating factor (attempt {attempt + 1}/{max_retries})...",
                            "attempt": attempt + 1,
                            "indicator_summary": indicator_summary,
                        },
                    )

                    if attempt == 0:
                        # é¦–æ¬¡å°è¯•ï¼šæ­£å¸¸ç”Ÿæˆ
                        generated = await agent.generate(
                            user_request=hypothesis,
                            factor_family=factor_family_enum,
                        )
                    else:
                        # åç»­å°è¯•ï¼šä½¿ç”¨é”™è¯¯åé¦ˆè¿›è¡Œæ”¹è¿›
                        logger.info(f"Refining factor (attempt {attempt + 1}): {last_error[:200] if last_error else 'No error'}...")
                        task.update_state(
                            state="PROGRESS",
                            meta={
                                "current": 30 + attempt * 15,
                                "total": 100,
                                "status": f"Refining factor based on feedback (attempt {attempt + 1}/{max_retries})...",
                                "attempt": attempt + 1,
                                "is_refining": True,
                            },
                        )
                        generated = await agent.refine(
                            original_code=last_code,
                            error_message=last_error,
                            user_request=hypothesis,
                            factor_family=factor_family_enum,
                        )

                    # éªŒè¯ç”Ÿæˆçš„å› å­å¯ä»¥è®¡ç®—ï¼ˆå¿«é€Ÿæ£€æŸ¥ï¼‰
                    try:
                        _validate_factor_computes(generated.code)
                    except Exception as compute_error:
                        # å› å­ç”ŸæˆæˆåŠŸä½†è®¡ç®—å¤±è´¥ï¼Œä¿å­˜é”™è¯¯ä¿¡æ¯ç”¨äºä¸‹æ¬¡æ”¹è¿›
                        last_code = generated.code
                        last_error = f"Factor computation failed: {str(compute_error)}"
                        logger.warning(f"Factor computation validation failed: {compute_error}")
                        if attempt < max_retries - 1:
                            continue
                        raise

                    # ===== æ™ºèƒ½æŒ‡æ ‡å®Œæ•´æ€§æ£€æŸ¥ =====
                    # Check if all requested indicators are implemented
                    is_complete, missing_feedback = check_factor_completeness(
                        hypothesis=hypothesis,
                        expression=generated.code,
                    )

                    if not is_complete and attempt < max_retries - 1:
                        # æŒ‡æ ‡ä¸å®Œæ•´ï¼Œæä¾›å…·ä½“åé¦ˆè®©LLMæ”¹è¿›
                        analysis = analyze_indicator_coverage(hypothesis, generated.code)
                        logger.warning(
                            f"Indicator coverage incomplete: "
                            f"requested={analysis.requested}, "
                            f"found={analysis.found}, "
                            f"missing={analysis.missing}"
                        )
                        task.update_state(
                            state="PROGRESS",
                            meta={
                                "current": 30 + attempt * 15,
                                "total": 100,
                                "status": f"Missing indicators: {', '.join(analysis.missing)}. Refining...",
                                "attempt": attempt + 1,
                                "missing_indicators": list(analysis.missing),
                            },
                        )
                        last_code = generated.code
                        last_error = missing_feedback
                        continue  # Retry with specific feedback

                    # Log indicator analysis for successful generation
                    analysis = analyze_indicator_coverage(hypothesis, generated.code)
                    logger.info(
                        f"Factor generated with indicators: {analysis.found}, "
                        f"completion rate: {analysis.completion_rate:.0%}"
                    )

                    task.update_state(
                        state="PROGRESS",
                        meta={
                            "current": 85,
                            "total": 100,
                            "status": "Factor validated successfully, saving...",
                        },
                    )

                    return {
                        "factor_id": str(uuid.uuid4()),
                        "hypothesis": hypothesis,
                        "family": generated.family.value,
                        "name": generated.name,
                        "description": generated.description,
                        "code": generated.code,
                        "metadata": {
                            **generated.metadata,
                            "attempts": attempt + 1,
                            "refined": attempt > 0,
                            "indicators_requested": list(analysis.requested),
                            "indicators_found": list(analysis.found),
                            "indicator_completion_rate": analysis.completion_rate,
                        },
                    }

                except (FactorGenerationError, InvalidFactorError) as e:
                    # ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é”™è¯¯ï¼Œä¿å­˜ç”¨äºä¸‹æ¬¡æ”¹è¿›
                    last_error = str(e)
                    last_code = getattr(e, 'code', last_code) or "# Previous code unavailable"
                    logger.warning(f"Factor generation attempt {attempt + 1} failed: {e}")
                    if attempt >= max_retries - 1:
                        raise

            # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä½œä¸ºå®‰å…¨ä¿éšœ
            raise FactorGenerationError(f"Max retries ({max_retries}) exceeded. Last error: {last_error}")

    try:
        # åœ¨ Celery åŒæ­¥ä»»åŠ¡ä¸­è¿è¡Œå¼‚æ­¥ä»£ç 
        result = asyncio.run(_generate_with_feedback_loop())
        logger.info(f"Factor generated successfully: {result.get('name')} (attempts: {result.get('metadata', {}).get('attempts', 1)})")
        return result

    except FactorGenerationError as e:
        logger.error(f"Factor generation failed after all retries: {e}")
        raise TaskError(f"Factor generation failed: {e}")
    except Exception as e:
        logger.error(f"Unexpected error in factor generation: {e}")
        raise TaskError(f"Factor generation error: {e}")


def _validate_factor_computes(code: str) -> bool:
    """Quick validation that factor expression can be computed.

    Args:
        code: Qlib expression or Python code

    Returns:
        True if valid

    Raises:
        Exception if validation fails
    """
    # For Qlib expressions, do a quick syntax check
    if "$" in code or any(op in code for op in ["Mean(", "Std(", "Ref(", "RSI(", "MACD("]):
        # Check for invalid fields
        invalid_fields = ["$returns", "$quote_volume", "$funding_rate", "$open_interest"]
        for invalid in invalid_fields:
            if invalid in code:
                raise ValueError(f"Invalid field used: {invalid}. Only $open, $high, $low, $close, $volume are allowed.")

        # Check for balanced parentheses
        paren_count = 0
        for char in code:
            if char == "(":
                paren_count += 1
            elif char == ")":
                paren_count -= 1
            if paren_count < 0:
                raise ValueError("Unbalanced parentheses: extra ')'")
        if paren_count != 0:
            raise ValueError("Unbalanced parentheses: missing ')'")

        return True

    # For Python code, try to parse it
    import ast
    try:
        ast.parse(code)
        return True
    except SyntaxError as e:
        raise ValueError(f"Python syntax error: {e}")


# ==================== Mining Task (C2 Fix: Persistent) ====================


@celery_app.task(
    bind=True,
    name="iqfmp.celery_app.tasks.mining_task",
    max_retries=1,
    default_retry_delay=60,
    acks_late=True,
    track_started=True,
    priority=5,
    queue="default",
)
def mining_task(
    self,
    task_id: str,
    task_config: dict[str, Any],
) -> dict[str, Any]:
    """
    å› å­æŒ–æ˜ä»»åŠ¡ - æŒä¹…åŒ–åˆ° Redis (C2 Fix)

    æ­¤ä»»åŠ¡é€šè¿‡ Celery æ‰§è¡Œï¼Œç¡®ä¿ï¼š
    1. ä»»åŠ¡æŒä¹…åŒ–åˆ° Redisï¼ŒæœåŠ¡é‡å¯åå¯æ¢å¤
    2. æ”¯æŒä»»åŠ¡å–æ¶ˆæ£€æŸ¥
    3. è¿›åº¦å®æ—¶æ›´æ–°

    Args:
        task_id: æŒ–æ˜ä»»åŠ¡ ID
        task_config: ä»»åŠ¡é…ç½®
            - name: ä»»åŠ¡åç§°
            - target_count: ç›®æ ‡å› å­æ•°é‡
            - factor_families: å› å­å®¶æ—åˆ—è¡¨
            - auto_evaluate: æ˜¯å¦è‡ªåŠ¨è¯„ä¼°

    Returns:
        æŒ–æ˜ç»“æœ
            - task_id: ä»»åŠ¡ ID
            - status: å®ŒæˆçŠ¶æ€
            - generated_count: ç”Ÿæˆçš„å› å­æ•°
            - passed_count: é€šè¿‡è¯„ä¼°çš„å› å­æ•°
            - failed_count: å¤±è´¥çš„å› å­æ•°
    """
    celery_task_id = self.request.id
    logger.info(f"Starting mining task {task_id} (Celery ID: {celery_task_id})")

    generated_count = 0
    passed_count = 0
    failed_count = 0

    try:
        # Update initial state
        self.update_state(
            state="PROGRESS",
            meta={
                "current": 0,
                "total": 100,
                "status": "Initializing mining task...",
                "started_at": datetime.utcnow().isoformat(),
                "task_id": task_id,
            },
        )

        target_count = task_config.get("target_count", 10)
        families = task_config.get("factor_families") or ["momentum", "volatility"]
        auto_evaluate = task_config.get("auto_evaluate", True)
        task_name = task_config.get("name", "Mining Task")
        # Use description as the primary hypothesis for factor generation
        # If no description, fall back to task_name
        task_description = task_config.get("description") or task_name

        logger.info(f"Mining task config: name='{task_name}', description='{task_description}'")

        # Run mining loop
        result = _execute_mining_task(
            celery_task=self,
            task_id=task_id,
            task_name=task_name,
            task_description=task_description,  # Pass description for factor generation
            target_count=target_count,
            families=families,
            auto_evaluate=auto_evaluate,
            data_config=task_config.get("data_config"),
        )

        logger.info(f"Mining task {task_id} completed: {result}")
        return {
            "task_id": task_id,
            "celery_task_id": celery_task_id,
            "status": "completed",
            "completed_at": datetime.utcnow().isoformat(),
            **result,
        }

    except Exception as e:
        logger.error(f"Mining task {task_id} failed: {e}")
        return {
            "task_id": task_id,
            "celery_task_id": celery_task_id,
            "status": "failed",
            "error": str(e),
            "generated_count": generated_count,
            "passed_count": passed_count,
            "failed_count": failed_count,
        }


def _execute_mining_task(
    celery_task,
    task_id: str,
    task_name: str,
    task_description: str,
    target_count: int,
    families: list[str],
    auto_evaluate: bool,
    data_config: dict | None = None,
) -> dict[str, Any]:
    """æ‰§è¡Œå› å­æŒ–æ˜çš„å†…éƒ¨å®ç°.

    æ­¤å‡½æ•°åœ¨ Celery Worker ä¸­åŒæ­¥æ‰§è¡Œã€‚
    ä½¿ç”¨æ•°æ®åº“è¿æ¥è¿›è¡Œå› å­ç”Ÿæˆå’Œè¯„ä¼°ã€‚

    Args:
        task_description: ç”¨æˆ·è¾“å…¥çš„å› å­æè¿°ï¼Œç”¨ä½œ LLM çš„ä¸»è¦æŒ‡ä»¤
    """
    import asyncio
    import time

    generated_count = 0
    passed_count = 0
    failed_count = 0

    # Cache redis client for cancellation checks (avoid reconnect per-iteration)
    redis_client = None
    try:
        import os
        import redis

        redis_url = os.getenv("REDIS_URL", "redis://localhost:6379/0")
        redis_client = redis.from_url(redis_url)
    except Exception:
        redis_client = None

    for i in range(target_count):
        # Check for cancellation via Redis
        if redis_client is not None:
            try:
                if redis_client.sismember("mining_tasks:cancelled", task_id):
                    logger.info(f"Mining task {task_id} cancelled at iteration {i}")
                    return {
                        "generated_count": generated_count,
                        "passed_count": passed_count,
                        "failed_count": failed_count,
                        "cancelled": True,
                    }
            except Exception:
                pass  # Redis check failed, continue execution

        # Update progress
        progress = ((i + 1) / target_count) * 100
        celery_task.update_state(
            state="PROGRESS",
            meta={
                "current": int(progress),
                "total": 100,
                "status": f"Generating factor {i + 1}/{target_count}...",
                "generated_count": generated_count,
                "passed_count": passed_count,
                "failed_count": failed_count,
            },
        )

        # Select family for this iteration
        family = families[i % len(families)]

        # Use task_description as the primary factor generation instruction
        # If description contains specific indicators, use it directly
        indicator_keywords = ["macd", "rsi", "ema", "sma", "wr", "ssl", "zigzag", "ziggy",
                              "bollinger", "atr", "adx", "cci", "momentum", "ç»“åˆ", "combine",
                              "ç­–ç•¥", "æŒ‡æ ‡", "williams", "stochastic", "ichimoku", "è¶‹åŠ¿",
                              "å‡çº¿", "çªç ´", "åè½¬", "è¶…ä¹°", "è¶…å–"]
        is_specific_request = any(keyword in task_description.lower() for keyword in indicator_keywords)

        logger.info(f"DEBUG task_description='{task_description}', is_specific={is_specific_request}")

        if is_specific_request:
            # User provided specific indicator description - use it directly as LLM instruction
            description = f"{task_description} (variant #{i+1}, family: {family})"
        else:
            # Generic description - use auto-generated prompt
            description = f"Auto-generated {family} factor #{i+1}: {task_description}"

        logger.info(f"DEBUG hypothesis='{description}'")

        try:
            # Generate factor using sync implementation
            factor_result = _execute_factor_generation(
                celery_task,
                hypothesis=description,
                factor_family=family,
                config={},
            )
            generated_count += 1

            # Auto-evaluate if enabled
            if auto_evaluate:
                try:
                    eval_result = _execute_factor_evaluation(
                        celery_task,
                        factor_id=factor_result.get("factor_id", "unknown"),
                        factor_code=factor_result.get("code", ""),
                        config={
                            "n_trials": generated_count,
                            "data_config": data_config or {},
                        },
                    )
                    sharpe = eval_result.get("sharpe", 0.0)
                    threshold = eval_result.get("threshold_used", 0.0)
                    passed = eval_result.get("passed_threshold", False)
                    logger.info(f"Factor evaluation: sharpe={sharpe:.4f}, threshold={threshold:.4f}, passed={passed}")

                    if passed:
                        passed_count += 1
                    else:
                        failed_count += 1
                except Exception as e:
                    logger.warning(f"Factor evaluation failed with exception: {e}")
                    import traceback
                    logger.warning(f"Traceback: {traceback.format_exc()}")
                    failed_count += 1

        except Exception as e:
            logger.error(f"Factor generation failed: {e}")
            failed_count += 1

        # Small delay to avoid overwhelming resources
        time.sleep(0.3)

    return {
        "generated_count": generated_count,
        "passed_count": passed_count,
        "failed_count": failed_count,
        "cancelled": False,
    }
